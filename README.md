# multimodal-matchmap
   
<div align="center">    
 
# Multimodal Matchmap    
</div>
 
## Description   
A Pytorch implementation of [Emotion recognition using multimodal matchmap fusion and multi-task learning](https://digital-library.theiet.org/content/conferences/10.1049/icp.2021.1454;jsessionid=2u5nnfgob1ipf.x-iet-live-01).

## Data   
This model uses the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset:
https://sail.usc.edu/iemocap/

## Model Training   
Run the file train.py. This file already has some defaults for general training.
You will need two CSV files, one for the video and another for the audio modalities. Each must contain the location and labels of the file.
